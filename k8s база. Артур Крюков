---- k8s начало. Артур Крюков [Изучаем Kubernetes с нуля. База] - https://boosty.to/bigkaa/posts/f901cdeb-b9f8-46c9-a411-58e3ca03ba93

Если вы начинаете изучать что-то - начните с документации. 
У кубера хорошая документация и даже местами русифицированная. 

- Зачем вам Kubernetes и что он может делать? 
Kubernetes - оркестратор над контейнерами. Он потребуется тогда, когда приложение состоит из контейнеров, когда у вас много контейнеров или много микросервисов в контейнерах. Основное слово - контейнеры. 

- Что он может делать?  
Задача Kubernetes:  
	- запускать и останавливать контейнеры
	- если ваше приложение закончило работу - перезапустить его 
	- если нода в кластере умрет, перезапустить на другой ноде
	- если ваше приложение потребляет много ресурсов, и вы это предусмотрели, то оно может автоматически горизонтально масштабировать приложение
	
Управление в k8s происходит с помощью манифестов. О том что это такое мы поговорим позже. Это какие-то файлы (условно) на языке YAML. В YAML/Манифестах мы описываем состояние системы (то есть тоже декларативная история, как и Ansible) и отдаем это k8s, а дальше он должен поддерживать это состояние. 

- Компоненты Kubernetes, как они называются, для чего они служат и как взаимодействуют. 

Основной компонент - K8S API - приложение, которое работает по протоколу Rest 

Rest API - приложение, которое дает вам доступ по http/https и дальше с помощью стандартных запросов (GET,POST и т.д.) можете общаться с приложением. 

Длят того чтобы работать с данными, пользователь/приложение обращается к Kubernetes API и с помощью комант управляет Kuberbetes`ом. Это некая входная точка для управления вашим кубером. 

Само по себе это приложение, которое ничего в себе не хранит. Соответственно, должна быть какая-то БД, где будут храниться соответствующие настройки/пожелания. По умолчанию используюется etcd. 

Etcd - БД простейшая ключ-значение. Обычно это не одно приложение, а какой-то кластер БД, в которых хранятся данные нашего кластера kubernetes. И когда мы обращаемся к API K8S, мы обращаемся в etcd за этими данными.

Controller - компонент, который реагирует на появление манифестов. Следит за появлением манифестов в системе. Следит не обращаясь к etcd, т.к. он о ней ничего не знает. Он смотрит на K8S API и смотрит появился ли в системе какой-нибудь манифест. Использует функцию watch, то есть он с определнной периодичностью к K8S API  и наблюдает появился ли там манифест.

То есть задача контроллера, добавить в K8S API, в соответствующую БД требование (!) на создание приложения. 
В манифесте мы выдаем пожелание, на этот манифест реагирует контроллер и потом создает уже требование на создание пода в API. 

Scheduler (Планировщик) - он точно также обращается в k8s API и смотрит появился ли там запрос на создание пода. 
Controller сформировал этот запрос, а Scheduler он уже следит за появлением запросов на создание пода и в зависимости от доступных ресурсов в кластере, от требований у пода, какие ресурсы доступны и т.д.. Он в деклапрацию этого пода помещает информацию о том где разместить этот под.   

kubelet - приложение, которое будет запускать наши поды, которые у нас есть в k8s api и следит за манифестами подов, где уже есть информация от планировщика, где уже есть информация о том на каком сервере запустить под.
По сути на кажом сервере в кластере запускается kubelet. Точно также с помощью watch следит за "обновлением" в k8s api.

kubelet обращается к системе контейнеризации, он смотрит а запущен ли такой под у него в системе, если не запущено, то он дает сообщение создать контейнер. И дальше система контейнерезации создает контейнер.  

k-proxy - он как и kubelet запускается на каждой ноде кластера kubernetes. этот компонент отвечает за сеть. сеть делить на: сеть подов и сеть сервисов. можно обратиться к подов по сети напрямую, зная его ip адресс. есть еще сущность services, который занимается распределением трафика между подами этого приложения. так вот, k-proxy нужен для того чтобы рулить сетью сервисов.

Итого у нас есть: K8S API, Controller, Scheduler, kubelet, k-proxy (etcd разрабатывает не гугл) 

Control Plane -  план управления кластером, по-умолчанию есть в любом кластере кубернетес. 

У нас также есть дополнительные компоненты (!): 
- DNS, встроенный в кубере, отдельный проект, встроенный в кубер. Эта история тоже работает с k8s api 
- CNI, драйвер сети подов на базе Container Network Interface. Тоже умеет работать с k8s api. 
Далее идут необязательные компоненты: 
- CSI, Container Storage Interface, k8s позволяет приложениям сохранять свои данные в каких-то постоянных хранилищах и за то, какие хранилища можно использовать - определяется драйвером. То есть CSI должен определять драйвер. 
- Additional Controller, который позволяет реализовывать какие-то дополнительные сущности, которые можно реализовывать в кластере k8s. Пример такой сущности - cert_manager 

kubelet - обычное приложение Linux, запускается с помощью дефолтного systemd, все остальное - поды в kubernetes 
k-proxy - отвечает за NAT преобразование.

Есть разбиение на Control Node и Worker Node. Contorol Node содержит элементы Control Plane нашего кластера. Там у нас находится etcd, там же рядом обычно ставится k8s api (обычно обращается к локальной etcd). Все 5 элементов (см. выше) находятся на Control Node (их должно быть несколько, нечетное кол-во (?). 

Etcd на разных Contorol Node связаны друг с другом и образуют кластер баз etcd.
